{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JgAANQLNyE3p"
   },
   "source": [
    "# Rnn and Sequence Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0vysAG7fyIQJ"
   },
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HEqu0hg8DaEh"
   },
   "source": [
    "\n",
    "## Predict a last character of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uw5s2SVGU9qI"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "char_arr = ['a', 'b', 'c', 'd', 'e', 'f', 'g',\n",
    "            'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
    "            'o', 'p', 'q', 'r', 's', 't', 'u',\n",
    "            'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "# one-hot encoding and decoding \n",
    "# {'a': 0, 'b': 1, 'c': 2, ..., 'j': 9, 'k', 10, ...}\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "dic_len = len(num_dic)\n",
    "\n",
    "\n",
    "# a list words for sequence data (input and output)\n",
    "seq_data = ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind']\n",
    "\n",
    "# Make a batch to have sequence data for input and ouput\n",
    "# wor -> X, d -> Y\n",
    "# dee -> X, p -> Y\n",
    "def make_batch(seq_data):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    for seq in seq_data:\n",
    "        # input data is:\n",
    "        #     wor           woo        dee       div\n",
    "        # [22, 14, 17] [22, 14, 14] [3, 4, 4] [3, 8, 21] ...\n",
    "        \n",
    "        input_data = [num_dic[n] for n in seq[:-1]]\n",
    "       \n",
    "        # target is :\n",
    "        # d, d, p, e, ...\n",
    "        # 3, 3, 15, 4, ...\n",
    "        target = num_dic[seq[-1]]\n",
    "        \n",
    "        # convert input to one-hot encoding.\n",
    "        # if input is [3, 4, 4]:\n",
    "        # [[ 0,  0,  0,  1,  0,  0,  0, ... 0]\n",
    "        #  [ 0,  0,  0,  0,  1,  0,  0, ... 0]\n",
    "        #  [ 0,  0,  0,  0,  1,  0,  0, ... 0]]\n",
    "        input_batch.append(np.eye(dic_len)[input_data])\n",
    "        \n",
    "        # sparse_softmax_cross_entropy_with_logits() will be used for cost function, does not require to convert to one-hot vector\n",
    "        target_batch.append(target)\n",
    "\n",
    "    return input_batch, target_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MwcYilPL1jK0"
   },
   "source": [
    "### **[softmax_cross_entropy_with_logits_v2()](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2) vs [sparse_softmax_cross_entropy_with_logits()](https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits) **\n",
    "\n",
    "**softmax_cross_entropy_with_logits_v2()**: While the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of labels is a valid probability distribution. If they are not, the computation of the gradient will be incorrect.\n",
    "\n",
    "**sparse_softmax_cross_entropy_with_logits()**: For this operation, the probability of a given label is considered exclusive. That is, soft classes are not allowed, and the labels vector must provide a single specific index for the true class for each row of logits (each minibatch entry). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9hp5TkPM1gxa"
   },
   "outputs": [],
   "source": [
    "### Setting hyperparameters\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_hidden = 128\n",
    "total_epoch = 50\n",
    "\n",
    "# Number of sequences for RNN\n",
    "n_step = 3\n",
    "\n",
    "# number of inputs (dimension of input vector) = 26\n",
    "n_input = dic_len\n",
    "# number of classes = 26\n",
    "n_class = dic_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "cfcWRs-C4zcr",
    "outputId": "1f83c96b-a531-42f9-a21f-d122ada978f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-3-288322d37a5d>:14: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-3-288322d37a5d>:20: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-3-288322d37a5d>:24: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "### Neural Network Model\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_step, n_input])\n",
    "\n",
    "# Again, here we are using sparse_softmax_cross_entropy_with_logits() for cost function, so the output is not one-hot vector\n",
    "# if we are getting one-hot vector shape should be: [None, n_class]\n",
    "Y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "\n",
    "W = tf.Variable(tf.random_normal([n_hidden, n_class]))\n",
    "b = tf.Variable(tf.random_normal([n_class]))\n",
    "\n",
    "# Create a cell for RNN \n",
    "cell1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "\n",
    "# Apply Dropout to prevent overfitting\n",
    "cell1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=0.5)\n",
    "cell2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "# RNN cell composed sequentially of multiple simple cells\n",
    "multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
    "\n",
    "# tf.nn.dynamic_rnn \n",
    "# time_major=True\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)\n",
    "\n",
    "# Convert output to one-hot vector\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "outputs = outputs[-1]\n",
    "model = tf.matmul(outputs, W) + b\n",
    "\n",
    "cost = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=model, labels=Y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n0NPv3hP7Oou"
   },
   "source": [
    "### Dropout\n",
    "[DropoutWrapper](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/DropoutWrapper)\n",
    "\n",
    "Dropout makes each hidden unit more robust and drive it towards creating useful features on its own without relying on other hidden units to correct its mistakes\n",
    "\n",
    "![dropout](https://cdn-images-1.medium.com/max/800/1*D8jriroKkjno8RztHKmMnA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 978
    },
    "colab_type": "code",
    "id": "lZrwHQ3g4oAK",
    "outputId": "af105dca-deee-4bc1-fd5c-4cc75686620e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 4.788125\n",
      "Epoch: 0002 cost = 3.654046\n",
      "Epoch: 0003 cost = 1.815120\n",
      "Epoch: 0004 cost = 1.920150\n",
      "Epoch: 0005 cost = 1.128462\n",
      "Epoch: 0006 cost = 1.063209\n",
      "Epoch: 0007 cost = 1.007574\n",
      "Epoch: 0008 cost = 0.652944\n",
      "Epoch: 0009 cost = 0.556815\n",
      "Epoch: 0010 cost = 0.563147\n",
      "Epoch: 0011 cost = 0.523048\n",
      "Epoch: 0012 cost = 0.318062\n",
      "Epoch: 0013 cost = 0.343976\n",
      "Epoch: 0014 cost = 0.397256\n",
      "Epoch: 0015 cost = 0.436182\n",
      "Epoch: 0016 cost = 0.403633\n",
      "Epoch: 0017 cost = 0.213721\n",
      "Epoch: 0018 cost = 0.204077\n",
      "Epoch: 0019 cost = 0.103348\n",
      "Epoch: 0020 cost = 0.124786\n",
      "Epoch: 0021 cost = 0.204780\n",
      "Epoch: 0022 cost = 0.172683\n",
      "Epoch: 0023 cost = 0.193289\n",
      "Epoch: 0024 cost = 0.207323\n",
      "Epoch: 0025 cost = 0.104241\n",
      "Epoch: 0026 cost = 0.043341\n",
      "Epoch: 0027 cost = 0.066465\n",
      "Epoch: 0028 cost = 0.073828\n",
      "Epoch: 0029 cost = 0.065845\n",
      "Epoch: 0030 cost = 0.085946\n",
      "Epoch: 0031 cost = 0.082732\n",
      "Epoch: 0032 cost = 0.055621\n",
      "Epoch: 0033 cost = 0.149330\n",
      "Epoch: 0034 cost = 0.027077\n",
      "Epoch: 0035 cost = 0.015617\n",
      "Epoch: 0036 cost = 0.023312\n",
      "Epoch: 0037 cost = 0.010602\n",
      "Epoch: 0038 cost = 0.012971\n",
      "Epoch: 0039 cost = 0.048157\n",
      "Epoch: 0040 cost = 0.084252\n",
      "Epoch: 0041 cost = 0.079431\n",
      "Epoch: 0042 cost = 0.126722\n",
      "Epoch: 0043 cost = 0.005565\n",
      "Epoch: 0044 cost = 0.005210\n",
      "Epoch: 0045 cost = 0.014615\n",
      "Epoch: 0046 cost = 0.060630\n",
      "Epoch: 0047 cost = 0.009947\n",
      "Epoch: 0048 cost = 0.008467\n",
      "Epoch: 0049 cost = 0.112413\n",
      "Epoch: 0050 cost = 0.004323\n",
      "Training completed\n",
      "\n",
      "=== Prediction Result ===\n",
      "Input: ['wor ', 'woo ', 'dee ', 'div ', 'col ', 'coo ', 'loa ', 'lov ', 'kis ', 'kin ']\n",
      "Predicted: ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind']\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "### Train Model\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "input_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    _, loss = sess.run([optimizer, cost],\n",
    "                       feed_dict={X: input_batch, Y: target_batch})\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1),\n",
    "          'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "print('Training completed')\n",
    "\n",
    "### Prediction Result\n",
    "# Convert predict result to integer\n",
    "prediction = tf.cast(tf.argmax(model, 1), tf.int32)\n",
    "# Compare predicted result with actual result\n",
    "prediction_check = tf.equal(prediction, Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction_check, tf.float32))\n",
    "\n",
    "input_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "predict, accuracy_val = sess.run([prediction, accuracy],\n",
    "                                 feed_dict={X: input_batch, Y: target_batch})\n",
    "\n",
    "predict_words = []\n",
    "for idx, val in enumerate(seq_data):\n",
    "    last_char = char_arr[predict[idx]]\n",
    "    predict_words.append(val[:3] + last_char)\n",
    "\n",
    "print('\\n=== Prediction Result ===')\n",
    "print('Input:', [w[:3] + ' ' for w in seq_data])\n",
    "print('Predicted:', predict_words)\n",
    "print('Accuracy:', accuracy_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KPDUdScFPRzT"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XfRXWwL2D9Bn"
   },
   "source": [
    "# Seq2Seq Model (N to M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UFRHe5J9c3QG"
   },
   "source": [
    "We are going to implement a sequence to sequence model that translates playing card symbols (Ace, Jack, Queen, King) to their associated number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1yEhcJZxxWvu"
   },
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "MyTFAjAYlIgn",
    "outputId": "3ff53897-f710-4a01-e829-2f8d9f9faffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B01\n",
      "B11\n",
      "B12\n",
      "B13\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Sequence data\n",
    "seq_data = [['ace', '01'], ['jack', '11'],\n",
    "            ['queen', '12'], ['king', '13']]\n",
    "\n",
    "# Generate unique tokens list\n",
    "chars = []\n",
    "for seq in seq_data:    \n",
    "    chars += list(seq[0])\n",
    "    chars += list(seq[1])\n",
    "    print('B'+ seq[1])\n",
    "char_arr = list(set(chars))\n",
    "\n",
    "# special tokens are required\n",
    "# B: Beginning of Sequence\n",
    "# E: Ending of Sequence\n",
    "# P: Padding of Sequence - for different size input\n",
    "# U: Unknown element of Sequence - for different size input\n",
    "char_arr.append('B')\n",
    "char_arr.append('E')\n",
    "char_arr.append('P')\n",
    "char_arr.append('U')\n",
    "\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "\n",
    "dic_len = len(num_dic)\n",
    "\n",
    "max_input_words_amount = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "zDrIIa_z2xg9",
    "outputId": "1800d7e4-834f-4e28-f786-a6f1314e9f71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 2,\n",
       " '1': 10,\n",
       " '2': 4,\n",
       " '3': 13,\n",
       " 'B': 14,\n",
       " 'E': 15,\n",
       " 'P': 16,\n",
       " 'U': 17,\n",
       " 'a': 9,\n",
       " 'c': 1,\n",
       " 'e': 0,\n",
       " 'g': 6,\n",
       " 'i': 7,\n",
       " 'j': 8,\n",
       " 'k': 3,\n",
       " 'n': 12,\n",
       " 'q': 11,\n",
       " 'u': 5}"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gKk4ORyCxbjE"
   },
   "source": [
    "## Generate batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNPoWhYDIL71"
   },
   "outputs": [],
   "source": [
    "\n",
    "# add paddings if the word is shorter than the maximum number of words\n",
    "def add_paddings(word):\n",
    "    diff = 5 - len(word)\n",
    "    for x in range(diff):\n",
    "        word += \"P\"\n",
    "    return word\n",
    "    \n",
    "\n",
    "# generate a batch data for training/testing\n",
    "def make_batch(seq_data):\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        # Input for encoder cell, convert to vector\n",
    "        input_word = add_paddings(seq[0])\n",
    "        input_data = [num_dic[n] for n in input_word]\n",
    "        print(input_data)\n",
    "        # Input for decoder cell, Add 'B' at the beginning of the sequence data\n",
    "        output_data  = [num_dic[n] for n in ('B'+ seq[1])]\n",
    "        \n",
    "        # Output of decoder cell (Actual result), Add 'E' at the end of the sequence data\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "        \n",
    "        # Convert each character vector to one-hot encode data\n",
    "        input_batch.append(np.eye(dic_len)[input_data])\n",
    "        output_batch.append(np.eye(dic_len)[output_data])\n",
    "        \n",
    "        target_batch.append(target)\n",
    "\n",
    "    return input_batch, output_batch, target_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "US15fGgPxgBf"
   },
   "source": [
    "## Build training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "esUeOZ6KIQol"
   },
   "outputs": [],
   "source": [
    "### Neural Network Model\n",
    "\n",
    "### Setting Hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_hidden = 128\n",
    "total_epoch = 100\n",
    "\n",
    "n_class = dic_len\n",
    "n_input = dic_len\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# encoder/decoder shape = [batch size, time steps, input size]\n",
    "enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "\n",
    "# target shape = [batch size, time steps]\n",
    "targets = tf.placeholder(tf.int64, [None, None])\n",
    "\n",
    "\n",
    "# Encoder Cell\n",
    "with tf.variable_scope('encode'):\n",
    "    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
    "\n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n",
    "                                            dtype=tf.float32)\n",
    "# Decoder Cell\n",
    "with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "\n",
    "    # [IMPORTANT] Setting enc_states as inital_state of decoder cell\n",
    "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
    "                                            initial_state=enc_states,\n",
    "                                            dtype=tf.float32)\n",
    "\n",
    "model = tf.layers.dense(outputs, n_class, activation=None)\n",
    "\n",
    "cost = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=model, labels=targets))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L30V5F6jxqR-"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "colab_type": "code",
    "id": "yG7KGxcOxsCc",
    "outputId": "2ddae57c-2813-4e24-9f81-4c0fa2ec7f3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 1, 0, 16, 16]\n",
      "[8, 9, 1, 3, 16]\n",
      "[11, 5, 0, 0, 12]\n",
      "[3, 7, 12, 6, 16]\n",
      "[array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0.]]), array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0.]]), array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.]]), array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0.]])]\n",
      "Epoch: 0001 cost = 2.954110\n",
      "Epoch: 0011 cost = 0.004975\n",
      "Epoch: 0021 cost = 0.010623\n",
      "Epoch: 0031 cost = 0.000196\n",
      "Epoch: 0041 cost = 0.000212\n",
      "Epoch: 0051 cost = 0.000331\n",
      "Epoch: 0061 cost = 0.000143\n",
      "Epoch: 0071 cost = 0.000060\n",
      "Epoch: 0081 cost = 0.000677\n",
      "Epoch: 0091 cost = 0.000166\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "### Training Model\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Generate a batch data\n",
    "input_batch, output_batch, target_batch = make_batch(seq_data)\n",
    "print(output_batch)\n",
    "# print(target_batch)\n",
    "# for target_each in target_batch : \n",
    "#     print(len(target_each))\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    _, loss = sess.run([optimizer, cost],\n",
    "                       feed_dict={enc_input: input_batch,\n",
    "                                  dec_input: output_batch,\n",
    "                                  targets: target_batch})\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1),\n",
    "              'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "print('Training completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bXf-4bByxgh8"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "bT0TjoPqF7qa",
    "outputId": "5fb32186-90cf-432f-c039-b29399de07ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prediction result ===\n",
      "ace -> 01\n",
      "jack -> 11\n",
      "queen -> 12\n",
      "king -> 13\n"
     ]
    }
   ],
   "source": [
    "### Evaluation\n",
    "\n",
    "# Predict the result \n",
    "def predict(word):\n",
    "    # Setting each character of predicted as 'U' (Unknown) \n",
    "    # ['king', 'UU']\n",
    "    word = add_paddings(word)\n",
    "    \n",
    "    seq_data = [word, 'U' * 2]\n",
    "\n",
    "    input_batch, output_batch, target_batch = make_batch([seq_data])\n",
    "    \n",
    "    prediction = tf.argmax(model, 2)\n",
    "\n",
    "    result = sess.run(prediction,\n",
    "                      feed_dict={enc_input: input_batch,\n",
    "                                 dec_input: output_batch,\n",
    "                                 targets: target_batch})\n",
    "\n",
    "    # convert index number to actual character \n",
    "    decoded = [char_arr[i] for i in result[0]]\n",
    "\n",
    "    # Remove anything after 'E' \n",
    "    end = decoded.index('E')\n",
    "    translated = ''.join(decoded[:end])\n",
    "\n",
    "    return translated\n",
    "\n",
    "\n",
    "print('=== Prediction result ===')\n",
    "print('ace ->', predict('ace'))\n",
    "print('jack ->', predict('jack'))\n",
    "print('queen ->', predict('queen'))\n",
    "print('king ->', predict('king'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-316qjIt-zS"
   },
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ka3S9SD1uDl4"
   },
   "source": [
    "You are required to implement Seq2Seq model chatbot. We are going to use [Microsoft Personality Chat Datasets](https://github.com/Microsoft/BotBuilder-PersonalityChat/tree/63dd818cc22ed5a84f7b77c88076809c0b77a88d/CSharp/Datasets) (Google Drive id is provided). \n",
    "\n",
    "Use \"Question\" and \"Answer\" data in the tsv file. We will be implementing Many-to-One Seq2Seq model and feed word-based (tokenised) input rather than character based.\n",
    "\n",
    "Fill the blank to complete the program\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-hlDQBqsV1L"
   },
   "source": [
    "### Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "jvwyvmfDegwE",
    "outputId": "14e354e0-a10c-4a99-a712-4413a7f54d70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# You should submit \"ipynb\" file (You can download it from \"File\" > \"Download .ipynb\") to Canvas\n",
    "import json\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Code to download file into Colaboratory:\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "id = '1zMofHuFOuc6FJ3ndj19VO5lTyWGA0QDJ'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('qna_chitchat_the_professional.tsv')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JPC00ceeIGlW"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GcwnJS0BsiEo"
   },
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vQPN9-Sysluc"
   },
   "outputs": [],
   "source": [
    "# Sequence data\n",
    "# Generate unique tokens list from qas.json\n",
    "seq_data = []\n",
    "whole_words = []\n",
    "max_input_words_amount = 0\n",
    "max_output_words_amount = 1\n",
    "\n",
    "\n",
    "df = pd.read_csv('qna_chitchat_the_professional.tsv', sep=\"\\t\")\n",
    "\n",
    "\n",
    "    \n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    ###<You need to fill here>###\n",
    "    question = row['Question']\n",
    "    answer = row['Answer']\n",
    "    \n",
    "    seq_data.append([question,answer])\n",
    "    ###</You need to fill here>###\n",
    "    \n",
    "    \n",
    "    ###<You need to fill here>###\n",
    "for seq in seq_data:\n",
    "    # we need to tokenise question    \n",
    "    tokenized_q = word_tokenize(seq[0])\n",
    "    # we do not need to tokenise answer (because we implement N to One model)\n",
    "    # make a list with only one element (whole sentence)\n",
    "    tokenized_a = [seq[1]]\n",
    "   \n",
    "    ###</You need to fill here>###\n",
    "    \n",
    "    \n",
    "    # add question list and answer list (one element)\n",
    "    whole_words += tokenized_q\n",
    "    whole_words += tokenized_a\n",
    "    \n",
    "    # we need to decide the maximum size of input word tokens\n",
    "    max_input_words_amount = max(len(tokenized_q), max_input_words_amount)\n",
    "\n",
    "\n",
    "  # we now have a vacabulary list\n",
    "unique_words = list(set(whole_words))\n",
    "\n",
    "# adding special tokens in the vocabulary list    \n",
    "# _B_: Beginning of Sequence\n",
    "# _E_: Ending of Sequence\n",
    "# _P_: Padding of Sequence - for different size input\n",
    "# _U_: Unknown element of Sequence - for different size input\n",
    "\n",
    "unique_words.append('_B_')\n",
    "unique_words.append('_E_')\n",
    "unique_words.append('_P_')\n",
    "unique_words.append('_U_')\n",
    "\n",
    "\n",
    "num_dic = {n: i for i, n in enumerate(unique_words)}\n",
    "dic_len = len(num_dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OyOIENfAJuT-",
    "outputId": "92b50599-d92c-4cf1-c0ef-43b0958fbfd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrAL53HKufcG"
   },
   "source": [
    "## Generate batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n4vtcmaZufk2"
   },
   "outputs": [],
   "source": [
    "# get token index vector of questions and add paddings if the word is shorter than the maximum number of words\n",
    "def get_vectors_q(sentence):\n",
    "    \n",
    "    # tokenise the sentence\n",
    "    ###<You need to fill here>###\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    ###</You need to fill here>###    \n",
    "    \n",
    "    diff = max_input_words_amount - len(tokenized_sentence)\n",
    "    \n",
    "    # add paddings if the word is shorter than the maximum number of words    \n",
    "    for x in range(diff):\n",
    "        ###<You need to fill here>###\n",
    "        tokenized_sentence.append('_P_')\n",
    "        \n",
    "        ###</You need to fill here>###            \n",
    "        \n",
    "    ###<You need to fill here>###\n",
    "    data = tokens_to_ids(tokenized_sentence)\n",
    "    ###</You need to fill here>###      \n",
    "    \n",
    "        \n",
    "    return data\n",
    "\n",
    "# get token index vector of answer\n",
    "def get_vectors_a(sentence):    \n",
    "    tokenized_sentence = [sentence]\n",
    "    data = tokens_to_ids(tokenized_sentence)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "\n",
    "# convert tokens to index\n",
    "def tokens_to_ids(tokenized_sentence):\n",
    "    ids = []\n",
    "\n",
    "    for token in tokenized_sentence:\n",
    "        ###<You need to fill here>###\n",
    "        if token in num_dic:\n",
    "            token2id=num_dic[token]\n",
    "            ids.append(token2id)\n",
    "                     \n",
    "        ###</You need to fill here>###      \n",
    "\n",
    "    return ids\n",
    "\n",
    "\n",
    "# generate a batch data for training/testing\n",
    "def make_batch(seq_data):\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for seq in seq_data:        \n",
    "        # Input for encoder cell, convert question to vector\n",
    "        ###<You need to fill here>###\n",
    "        input_data = get_vectors_q(seq[0])\n",
    "        ###</You need to fill here>###      \n",
    "        print(input_data)  \n",
    "        # Input for decoder cell, Add '_B_' at the beginning of the sequence data\n",
    "        ###<You need to fill here>###\n",
    "        output_data= [num_dic['_B_']]\n",
    "        ###</You need to fill here>###   \n",
    "        output_data += get_vectors_a(seq[1])\n",
    "        \n",
    "        # Output of decoder cell (Actual result), Add '_E_' at the end of the sequence data\n",
    "        ###<You need to fill here>###\n",
    "        target = get_vectors_a(seq[1])\n",
    "        target.append(num_dic['_E_'])                           \n",
    "        ###</You need to fill here>###   \n",
    "         \n",
    "        \n",
    "        # Convert each token vector to one-hot encode data\n",
    "        input_batch.append(np.eye(dic_len)[input_data])\n",
    "        output_batch.append(np.eye(dic_len)[output_data])\n",
    "        \n",
    "        target_batch.append(target)\n",
    "   \n",
    "    return input_batch, output_batch, target_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r_W08vIPv7L7"
   },
   "source": [
    "## Build training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 10987
    },
    "colab_type": "code",
    "id": "R4o1hzIzuDvj",
    "outputId": "6131f6b9-d986-4401-be31-a4ae19544a53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107, 141, 19, 595, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 91, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[578, 606, 586, 4, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 595, 222, 586, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 382, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[104, 382, 222, 586, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[104, 442, 361, 606, 586, 4, 118, 685, 685, 685, 685, 685, 685]\n",
      "[506, 88, 381, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[525, 445, 583, 68, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[286, 586, 583, 88, 15, 381, 118, 685, 685, 685, 685, 685, 685]\n",
      "[8, 349, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 121, 192, 583, 19, 381, 615, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 547, 615, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[253, 553, 547, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[253, 553, 383, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[253, 141, 441, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[489, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[253, 141, 547, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[412, 174, 634, 141, 98, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[253, 141, 98, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[109, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[12, 174, 634, 141, 547, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[12, 174, 634, 141, 98, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[150, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[360, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[172, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[320, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 675, 294, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[423, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[189, 59, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[656, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[189, 59, 192, 586, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[189, 167, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[671, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[189, 603, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[363, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[51, 142, 592, 603, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[189, 603, 192, 586, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[420, 603, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[104, 553, 19, 254, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[104, 141, 19, 254, 665, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[196, 142, 592, 254, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[275, 192, 586, 271, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[60, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[52, 586, 271, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[258, 445, 388, 640, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[518, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[518, 353, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[351, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[280, 599, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[253, 553, 142, 462, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[190, 141, 443, 586, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 359, 191, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 359, 599, 106, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[104, 222, 586, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[104, 222, 586, 223, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[104, 222, 402, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[104, 222, 586, 277, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[104, 141, 19, 254, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 76, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[530, 141, 88, 403, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[537, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[525, 586, 112, 197, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[525, 586, 112, 197, 405, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[537, 487, 587, 355, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[576, 602, 154, 443, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[256, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[189, 167, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[272, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[601, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[162, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[272, 79, 309, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 114, 410, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[122, 202, 114, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 202, 317, 62, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[464, 202, 448, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 30, 202, 317, 592, 118, 685, 685, 685, 685, 685, 685]\n",
      "[104, 47, 114, 248, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 307, 586, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 307, 586, 414, 245, 309, 685, 685, 685, 685, 685, 685, 685]\n",
      "[520, 444, 5, 586, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 660, 197, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 444, 5, 586, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 414, 124, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[458, 207, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 379, 140, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[506, 129, 140, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[625, 222, 586, 414, 105, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 414, 105, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 105, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 73, 630, 485, 197, 163, 390, 685, 685, 685, 685, 685, 685]\n",
      "[625, 222, 586, 414, 105, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 259, 105, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 510, 238, 583, 129, 105, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 74, 510, 238, 583, 129, 103, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 318, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 222, 35, 309, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 682, 309, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 613, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 270, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 30, 586, 559, 16, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 242, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 35, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 630, 586, 35, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[104, 245, 129, 35, 490, 586, 486, 118, 685, 685, 685, 685, 685]\n",
      "[664, 222, 270, 158, 519, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 414, 270, 615, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[253, 553, 270, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[253, 141, 613, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 215, 615, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 142, 231, 615, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 621, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 213, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 149, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 79, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[45, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 106, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[431, 222, 586, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 410, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[104, 73, 202, 317, 223, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 209, 88, 646, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 73, 586, 30, 5, 197, 118, 685, 685, 685, 685, 685, 685]\n",
      "[464, 202, 142, 20, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[464, 202, 406, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 202, 317, 592, 185, 266, 118, 685, 685, 685, 685, 685, 685]\n",
      "[104, 73, 586, 365, 194, 500, 389, 118, 685, 685, 685, 685, 685]\n",
      "[107, 285, 477, 28, 192, 477, 316, 118, 685, 685, 685, 685, 685]\n",
      "[107, 285, 477, 292, 5, 65, 118, 685, 685, 685, 685, 685, 685]\n",
      "[107, 73, 586, 30, 194, 432, 118, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 30, 495, 222, 569, 118, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 39, 36, 488, 266, 118, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 209, 197, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 88, 231, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 505, 586, 209, 197, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 121, 586, 192, 209, 197, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 222, 142, 83, 309, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[104, 354, 222, 586, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 183, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[104, 183, 222, 586, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 354, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 584, 259, 354, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 259, 354, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 354, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 169, 142, 97, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[498, 163, 29, 354, 586, 222, 685, 685, 685, 685, 685, 685, 685]\n",
      "[104, 354, 586, 222, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 414, 354, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 173, 183, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[525, 586, 399, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 367, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 54, 574, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 630, 586, 203, 399, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 591, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 135, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 394, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 486, 444, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[525, 586, 628, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 19, 661, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 6, 202, 651, 586, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 54, 142, 661, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 73, 586, 509, 100, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[598, 222, 586, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[598, 285, 19, 543, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[598, 141, 19, 164, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[598, 141, 19, 373, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 54, 24, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 54, 331, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 54, 166, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[431, 9, 586, 366, 267, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[431, 73, 586, 366, 267, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 54, 142, 565, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[598, 285, 19, 556, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 54, 142, 234, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 54, 142, 233, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 54, 142, 164, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 54, 142, 373, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 19, 373, 141, 661, 118, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 19, 164, 141, 661, 118, 685, 685, 685, 685, 685, 685]\n",
      "[598, 141, 19, 41, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 313, 197, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 313, 88, 661, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 313, 467, 202, 114, 118, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 88, 661, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[598, 114, 202, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 285, 477, 292, 5, 65, 118, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 313, 477, 292, 5, 65, 118, 685, 685, 685, 685, 685]\n",
      "[107, 141, 477, 292, 5, 65, 118, 685, 685, 685, 685, 685, 685]\n",
      "[90, 630, 586, 486, 66, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 486, 66, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 203, 486, 66, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 73, 586, 564, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 629, 5, 1, 73, 586, 209, 118, 685, 685, 685, 685, 685]\n",
      "[90, 586, 564, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 66, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 209, 376, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 73, 586, 209, 192, 564, 118, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 19, 10, 350, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 19, 10, 144, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 19, 10, 212, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 19, 10, 579, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 19, 10, 1, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[598, 141, 19, 10, 438, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[598, 141, 19, 10, 544, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 19, 10, 58, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[321, 617, 226, 73, 586, 209, 118, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 209, 617, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 142, 231, 5, 69, 308, 118, 685, 685, 685, 685, 685]\n",
      "[107, 629, 5, 639, 73, 586, 209, 118, 685, 685, 685, 685, 685]\n",
      "[107, 350, 73, 586, 209, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 73, 586, 30, 194, 197, 118, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 19, 18, 5, 197, 118, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 88, 231, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[464, 202, 142, 592, 620, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[525, 586, 548, 325, 413, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[450, 630, 586, 229, 19, 148, 118, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 54, 160, 143, 252, 118, 685, 685, 685, 685, 685, 685]\n",
      "[625, 73, 586, 548, 477, 130, 154, 390, 477, 427, 118, 685, 685]\n",
      "[202, 305, 586, 611, 548, 310, 413, 685, 685, 685, 685, 685, 685]\n",
      "[664, 75, 555, 477, 130, 154, 390, 477, 427, 685, 685, 685, 685]\n",
      "[202, 114, 444, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 414, 214, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 359, 121, 192, 509, 192, 399, 685, 685, 685, 685, 685, 685]\n",
      "[37, 444, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 121, 192, 48, 568, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 121, 192, 380, 568, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 407, 644, 422, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 390, 679, 650, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 522, 197, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 238, 209, 197, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[625, 73, 586, 522, 197, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 30, 586, 522, 197, 615, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 290, 522, 197, 615, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 517, 372, 197, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 188, 86, 372, 197, 118, 685, 685, 685, 685, 685, 685]\n",
      "[598, 141, 517, 174, 197, 488, 586, 118, 685, 685, 685, 685, 685]\n",
      "[321, 85, 5, 507, 285, 129, 47, 118, 685, 685, 685, 685, 685]\n",
      "[664, 436, 379, 481, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 73, 586, 264, 100, 634, 118, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 238, 531, 481, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[253, 433, 630, 377, 481, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 73, 586, 13, 264, 100, 634, 118, 685, 685, 685, 685, 685]\n",
      "[107, 73, 586, 264, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 73, 630, 38, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[253, 436, 379, 481, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[23, 192, 377, 161, 481, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 73, 630, 486, 600, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 238, 116, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[286, 586, 551, 197, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 121, 192, 551, 586, 615, 685, 685, 685, 685, 685, 685, 685]\n",
      "[286, 586, 583, 88, 126, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 121, 586, 192, 583, 88, 333, 685, 685, 685, 685, 685, 685]\n",
      "[202, 121, 192, 170, 477, 312, 5, 88, 65, 282, 586, 685, 685]\n",
      "[107, 285, 670, 282, 586, 309, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 670, 282, 586, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 240, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 454, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 493, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[516, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 222, 493, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 454, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 222, 414, 326, 309, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 477, 278, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 222, 414, 31, 163, 602, 615, 685, 685, 685, 685, 685, 685]\n",
      "[664, 73, 630, 313, 325, 615, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 6, 202, 73, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[681, 202, 486, 142, 102, 424, 118, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 30, 202, 6, 418, 526, 650, 118, 685, 685, 685, 685]\n",
      "[90, 586, 30, 202, 6, 418, 408, 650, 118, 685, 685, 685, 685]\n",
      "[431, 6, 202, 509, 443, 558, 118, 685, 685, 685, 685, 685, 685]\n",
      "[681, 202, 395, 650, 644, 33, 118, 685, 685, 685, 685, 685, 685]\n",
      "[202, 329, 583, 529, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[255, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[669, 185, 142, 304, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[346, 443, 142, 61, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[525, 586, 371, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[55, 142, 212, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[51, 586, 203, 604, 142, 212, 118, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 203, 371, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 73, 586, 209, 192, 371, 15, 118, 685, 685, 685, 685, 685]\n",
      "[90, 586, 371, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[525, 586, 371, 142, 212, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 313, 160, 298, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 313, 160, 540, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[55, 142, 636, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[663, 142, 636, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[55, 310, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[525, 445, 347, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[275, 192, 197, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[525, 586, 677, 192, 197, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[275, 282, 197, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[456, 282, 197, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[525, 586, 347, 282, 197, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[84, 310, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[598, 141, 19, 580, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[598, 141, 19, 228, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 477, 661, 5, 19, 580, 118, 685, 685, 685, 685, 685]\n",
      "[107, 141, 19, 580, 425, 661, 118, 685, 685, 685, 685, 685, 685]\n",
      "[598, 73, 586, 211, 192, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[50, 197, 325, 615, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[50, 197, 142, 642, 615, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[525, 586, 418, 197, 142, 642, 118, 685, 685, 685, 685, 685, 685]\n",
      "[50, 197, 310, 615, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 73, 586, 121, 192, 313, 194, 197, 118, 685, 685, 685, 685]\n",
      "[662, 586, 142, 315, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 295, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 237, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 337, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 523, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 324, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 142, 344, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 168, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 378, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[431, 73, 586, 257, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[431, 222, 586, 267, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[431, 222, 586, 49, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 69, 222, 586, 185, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 132, 222, 586, 185, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 132, 222, 586, 267, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 69, 222, 586, 267, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[431, 141, 19, 461, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[431, 222, 586, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 222, 586, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 279, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 374, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 142, 620, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 142, 46, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[303, 488, 46, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[222, 586, 279, 488, 571, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 155, 586, 615, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 185, 155, 282, 586, 615, 685, 685, 685, 685, 685, 685]\n",
      "[577, 586, 309, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 222, 477, 155, 5, 88, 65, 685, 685, 685, 685, 685, 685]\n",
      "[202, 387, 586, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 114, 549, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 343, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 365, 414, 16, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 185, 169, 142, 592, 585, 685, 685, 685, 685, 685, 685]\n",
      "[446, 285, 592, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 490, 586, 73, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 490, 586, 293, 197, 282, 118, 685, 685, 685, 685, 685, 685]\n",
      "[107, 73, 586, 73, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 19, 345, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[104, 490, 586, 293, 197, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 64, 5, 402, 490, 586, 73, 118, 685, 685, 685, 685, 685]\n",
      "[662, 586, 88, 113, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 222, 88, 15, 381, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 88, 301, 381, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 88, 381, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 522, 197, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 73, 586, 30, 5, 197, 118, 685, 685, 685, 685, 685, 685]\n",
      "[202, 114, 238, 19, 381, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[53, 197, 142, 462, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[53, 142, 462, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[84, 142, 462, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[136, 197, 142, 462, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 313, 160, 11, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[104, 549, 222, 586, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 584, 549, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[104, 549, 222, 586, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 584, 259, 549, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 414, 549, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 630, 586, 273, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 630, 586, 57, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 549, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 259, 549, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 414, 263, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 263, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[463, 475, 197, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 114, 535, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[463, 610, 194, 197, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 305, 202, 606, 630, 414, 535, 685, 685, 685, 685, 685, 685]\n",
      "[202, 522, 586, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 334, 586, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 297, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 522, 87, 194, 586, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 482, 266, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 153, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 365, 539, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 414, 539, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 563, 5, 447, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 539, 223, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 259, 539, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 155, 88, 565, 615, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 155, 308, 615, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 185, 155, 615, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 155, 281, 139, 615, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 155, 157, 643, 615, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[609, 192, 388, 586, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[530, 141, 142, 403, 192, 388, 586, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 414, 385, 192, 388, 586, 685, 685, 685, 685, 685, 685]\n",
      "[530, 141, 259, 682, 192, 388, 586, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 313, 143, 470, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 313, 460, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 313, 384, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 313, 435, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 313, 678, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 313, 143, 432, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 68, 282, 143, 432, 118, 685, 685, 685, 685, 685, 685]\n",
      "[51, 586, 415, 435, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 657, 435, 572, 650, 118, 685, 685, 685, 685, 685, 685]\n",
      "[107, 143, 432, 73, 586, 313, 118, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 313, 143, 432, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 313, 143, 597, 274, 118, 685, 685, 685, 685, 685, 685]\n",
      "[107, 285, 294, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 294, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 102, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 81, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 222, 586, 294, 192, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 73, 586, 30, 194, 155, 118, 685, 685, 685, 685, 685, 685]\n",
      "[107, 285, 155, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 7, 185, 155, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 155, 82, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[598, 73, 586, 155, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 155, 197, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[53, 197, 29, 245, 586, 155, 197, 685, 685, 685, 685, 685, 685]\n",
      "[104, 245, 73, 586, 155, 197, 118, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 185, 155, 282, 197, 118, 685, 685, 685, 685, 685, 685]\n",
      "[202, 114, 66, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 246, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 335, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 121, 192, 564, 310, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 414, 66, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[392, 192, 541, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 222, 581, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 581, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 550, 110, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 169, 550, 110, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 142, 220, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 339, 294, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 414, 581, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[260, 533, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 478, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[37, 478, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[181, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 414, 478, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[221, 478, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 404, 630, 264, 634, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[652, 174, 478, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[283, 194, 634, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[311, 197, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[552, 197, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[552, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[311, 142, 356, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 330, 19, 17, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 142, 439, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 667, 443, 197, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 282, 477, 680, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 282, 477, 206, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 282, 477, 179, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 176, 88, 219, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 176, 88, 494, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 121, 192, 638, 477, 70, 118, 685, 685, 685, 685, 685]\n",
      "[662, 586, 200, 70, 468, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 477, 649, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 473, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 534, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 95, 480, 616, 615, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 317, 616, 615, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 222, 414, 238, 448, 615, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 375, 21, 615, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[22, 375, 285, 209, 142, 401, 615, 685, 685, 685, 685, 685, 685]\n",
      "[22, 375, 285, 616, 615, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 142, 440, 615, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[227, 615, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 121, 192, 509, 648, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 114, 665, 443, 142, 247, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 672, 142, 102, 42, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 430, 32, 547, 405, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 54, 409, 483, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 114, 268, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 490, 235, 142, 0, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 177, 630, 251, 644, 197, 289, 685, 685, 685, 685, 685, 685]\n",
      "[664, 222, 44, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 193, 202, 369, 472, 27, 54, 192, 56, 586, 509, 615]\n",
      "[664, 559, 44, 309, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 222, 379, 269, 14, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 411, 586, 142, 619, 218, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 472, 27, 583, 101, 626, 685, 685, 685, 685, 685, 685]\n",
      "[664, 222, 405, 101, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 444, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 114, 277, 634, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 114, 142, 396, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 114, 142, 459, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 142, 315, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 550, 416, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 267, 79, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[598, 499, 586, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[431, 404, 586, 366, 267, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[598, 436, 586, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[598, 285, 19, 637, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[321, 496, 436, 586, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[598, 524, 586, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[253, 141, 238, 270, 615, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 238, 270, 615, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[253, 553, 630, 270, 615, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[400, 270, 615, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 222, 414, 120, 615, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[253, 553, 142, 581, 28, 615, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 238, 195, 88, 642, 615, 685, 685, 685, 685, 685, 685]\n",
      "[253, 141, 414, 670, 615, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[253, 141, 238, 98, 615, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[253, 141, 393, 615, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 145, 533, 615, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[362, 586, 96, 197, 553, 291, 615, 685, 685, 685, 685, 685, 685]\n",
      "[253, 553, 630, 98, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[253, 141, 238, 441, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[546, 174, 634, 141, 238, 98, 685, 685, 685, 685, 685, 685, 685]\n",
      "[184, 174, 291, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[554, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[631, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[400, 98, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 114, 249, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 114, 635, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 632, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 99, 533, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 25, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 414, 434, 250, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[136, 197, 142, 623, 208, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[136, 197, 142, 511, 504, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[474, 504, 309, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[338, 208, 309, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[625, 238, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[625, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[625, 285, 634, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 71, 586, 30, 414, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 71, 586, 30, 634, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[625, 73, 586, 30, 634, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[262, 197, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 72, 142, 368, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 305, 202, 510, 368, 586, 685, 685, 685, 685, 685, 685, 685]\n",
      "[525, 202, 54, 142, 368, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[94, 352, 309, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[94, 512, 309, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[666, 618, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[94, 323, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[582, 192, 491, 309, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 155, 586, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 209, 586, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 30, 586, 559, 414, 448, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 169, 142, 119, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 9, 209, 192, 156, 586, 650, 443, 142, 332, 685, 685, 685]\n",
      "[202, 30, 586, 559, 465, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 209, 586, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 30, 586, 559, 528, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 477, 15, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 559, 414, 569, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[664, 222, 88, 10, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 19, 276, 231, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 142, 108, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 142, 40, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 142, 608, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 147, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 178, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 19, 588, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 142, 658, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 142, 93, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 142, 40, 488, 142, 608, 118, 685, 685, 685, 685, 685]\n",
      "[662, 586, 142, 93, 488, 142, 658, 118, 685, 685, 685, 685, 685]\n",
      "[662, 586, 147, 488, 178, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 19, 588, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[236, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[429, 309, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[437, 309, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[127, 592, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[570, 644, 197, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[225, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 133, 600, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[253, 141, 35, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[412, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[12, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[12, 192, 634, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[601, 678, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[601, 384, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[601, 435, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[601, 460, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[272, 678, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[272, 435, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[272, 384, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[272, 460, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[357, 197, 310, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 170, 427, 185, 19, 244, 118, 685, 685, 685, 685, 685]\n",
      "[104, 511, 490, 586, 199, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 426, 645, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[525, 586, 538, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[525, 586, 377, 197, 142, 284, 118, 685, 685, 685, 685, 685, 685]\n",
      "[525, 586, 674, 88, 261, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[525, 586, 138, 192, 142, 232, 118, 685, 685, 685, 685, 685, 685]\n",
      "[104, 511, 490, 586, 138, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[525, 586, 426, 115, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 73, 586, 30, 194, 358, 118, 685, 685, 685, 685, 685, 685]\n",
      "[107, 73, 586, 30, 194, 622, 118, 685, 685, 685, 685, 685, 685]\n",
      "[107, 73, 586, 30, 194, 432, 118, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 209, 514, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 142, 231, 5, 624, 118, 685, 685, 685, 685, 685, 685]\n",
      "[107, 606, 586, 277, 128, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 19, 424, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 73, 586, 73, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 404, 586, 73, 128, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 665, 443, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 222, 586, 277, 547, 405, 118, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 81, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 141, 294, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 222, 586, 277, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 222, 586, 277, 545, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 222, 586, 277, 271, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 404, 586, 73, 223, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[392, 370, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[508, 294, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[328, 309, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[457, 3, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[340, 174, 586, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[506, 476, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[594, 600, 309, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[578, 329, 586, 627, 294, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 305, 586, 359, 509, 370, 615, 685, 685, 685, 685, 685, 685]\n",
      "[625, 73, 630, 586, 203, 566, 3, 118, 685, 685, 685, 685, 685]\n",
      "[84, 310, 270, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[506, 270, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[84, 142, 180, 154, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[84, 310, 182, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[84, 310, 454, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[84, 310, 581, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[506, 180, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[506, 182, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 114, 596, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 414, 596, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[45, 141, 288, 192, 73, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 369, 596, 650, 5, 88, 261, 685, 685, 685, 685, 685, 685]\n",
      "[202, 177, 630, 30, 5, 325, 202, 121, 192, 73, 685, 685, 685]\n",
      "[201, 197, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[136, 197, 142, 567, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 72, 142, 567, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[122, 141, 142, 567, 644, 586, 685, 685, 685, 685, 685, 685, 685]\n",
      "[136, 197, 161, 417, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 73, 586, 30, 194, 435, 118, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 209, 435, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 73, 586, 30, 194, 384, 118, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 209, 384, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[107, 73, 586, 30, 194, 460, 118, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 209, 460, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 142, 231, 5, 460, 118, 685, 685, 685, 685, 685, 685]\n",
      "[662, 586, 146, 372, 197, 118, 685, 685, 685, 685, 685, 685, 685]\n",
      "[598, 141, 146, 174, 197, 488, 586, 118, 685, 685, 685, 685, 685]\n",
      "[321, 85, 5, 507, 285, 146, 118, 685, 685, 685, 685, 685, 685]\n",
      "[90, 586, 30, 586, 559, 146, 372, 197, 118, 685, 685, 685, 685]\n",
      "[319, 586, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[134, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[386, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[243, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 296, 600, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[319, 586, 414, 245, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[202, 575, 586, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "[655, 633, 224, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n"
     ]
    }
   ],
   "source": [
    "### Setting Hyperparameters\n",
    "learning_rate = 0.002\n",
    "n_hidden = 128\n",
    "\n",
    "n_class = dic_len\n",
    "n_input = dic_len\n",
    "\n",
    "### Neural Network Model\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# encoder/decoder shape = [batch size, time steps, input size]\n",
    "enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "\n",
    "# target shape = [batch size, time steps]\n",
    "targets = tf.placeholder(tf.int64, [None, None])\n",
    "\n",
    "\n",
    "# Encoder Cell\n",
    "with tf.variable_scope('encode'):\n",
    "    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
    "\n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n",
    "                                            dtype=tf.float32)\n",
    "# Decoder Cell\n",
    "with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "\n",
    "    # [IMPORTANT] Setting enc_states as inital_state of decoder cell\n",
    "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
    "                                            initial_state=enc_states,\n",
    "                                            dtype=tf.float32)\n",
    "\n",
    "model = tf.layers.dense(outputs, n_class, activation=None)\n",
    "\n",
    "cost = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=model, labels=targets))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    \n",
    "### Training Model\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Generate a batch data\n",
    "input_batch, output_batch, target_batch = make_batch(seq_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJIcvSB2w_rs"
   },
   "source": [
    "## Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "colab_type": "code",
    "id": "TxsUy2hQOvl4",
    "outputId": "be7f3cac-4686-463d-d05e-78be1ff8eec5"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-71e94b08c46b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     _, loss = sess.run([optimizer, cost],\n\u001b[0m\u001b[1;32m      5\u001b[0m                        feed_dict={enc_input: input_batch,\n\u001b[1;32m      6\u001b[0m                                   \u001b[0mdec_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sess' is not defined"
     ]
    }
   ],
   "source": [
    "total_epoch = 5000\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    _, loss = sess.run([optimizer, cost],\n",
    "                       feed_dict={enc_input: input_batch,\n",
    "                                  dec_input: output_batch,\n",
    "                                  targets: target_batch})\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1),\n",
    "              'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "\n",
    "print('Training completed')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RT2GVA2L04r4"
   },
   "source": [
    "\n",
    "### Output:\n",
    "```\n",
    "Epoch: 0001 cost = 6.520396\n",
    "Epoch: 0101 cost = 2.285382\n",
    "Epoch: 0201 cost = 2.246728\n",
    "Epoch: 0301 cost = 2.245017\n",
    "Epoch: 0401 cost = 2.246025\n",
    "Epoch: 0501 cost = 2.240861\n",
    "Epoch: 0601 cost = 2.243242\n",
    "Epoch: 0701 cost = 2.242677\n",
    "Epoch: 0801 cost = 2.237896\n",
    "Epoch: 0901 cost = 2.231663\n",
    "Epoch: 1001 cost = 2.235167\n",
    "Epoch: 1101 cost = 2.241715\n",
    "Epoch: 1201 cost = 2.231011\n",
    "Epoch: 1301 cost = 2.239064\n",
    "Epoch: 1401 cost = 2.239094\n",
    "Epoch: 1501 cost = 2.238503\n",
    "Epoch: 1601 cost = 2.240443\n",
    "Epoch: 1701 cost = 1.363001\n",
    "Epoch: 1801 cost = 1.003514\n",
    "Epoch: 1901 cost = 0.825378\n",
    "Epoch: 2001 cost = 0.740003\n",
    "Epoch: 2101 cost = 0.740753\n",
    "Epoch: 2201 cost = 0.584820\n",
    "Epoch: 2301 cost = 0.520495\n",
    "Epoch: 2401 cost = 0.475480\n",
    "Epoch: 2501 cost = 0.451525\n",
    "Epoch: 2601 cost = 0.447824\n",
    "Epoch: 2701 cost = 0.391890\n",
    "Epoch: 2801 cost = 0.372204\n",
    "Epoch: 2901 cost = 0.364561\n",
    "Epoch: 3001 cost = 0.325550\n",
    "Epoch: 3101 cost = 0.328401\n",
    "Epoch: 3201 cost = 0.278636\n",
    "Epoch: 3301 cost = 0.287389\n",
    "Epoch: 3401 cost = 0.242130\n",
    "Epoch: 3501 cost = 0.226778\n",
    "Epoch: 3601 cost = 0.197740\n",
    "Epoch: 3701 cost = 0.351003\n",
    "Epoch: 3801 cost = 0.170044\n",
    "Epoch: 3901 cost = 0.138550\n",
    "Epoch: 4001 cost = 0.139902\n",
    "Epoch: 4101 cost = 0.119088\n",
    "Epoch: 4201 cost = 0.111903\n",
    "Epoch: 4301 cost = 0.117569\n",
    "Epoch: 4401 cost = 0.115386\n",
    "Epoch: 4501 cost = 0.089837\n",
    "Epoch: 4601 cost = 0.107535\n",
    "Epoch: 4701 cost = 0.094628\n",
    "Epoch: 4801 cost = 0.079053\n",
    "Epoch: 4901 cost = 0.762524\n",
    "Epoch: 5000 cost = 0.083978\n",
    "Training completed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-OmjFDS5wICj"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "356ED2TFNMq7",
    "outputId": "6d467c6a-d6d0-4529-8cbc-5f76d9767b2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[601, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "Hello  -> Hello.\n",
      "[202, 114, 414, 263, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "I am so lonely  -> Maybe a snack will help.\n",
      "[525, 586, 399, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "Can you sleep?  -> I don't have a body.\n",
      "[107, 285, 19, 595, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "What is your age?  -> Age doesn't really apply to me. \n",
      "[202, 522, 586, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "I hate you  -> I'm sorry to hear that.\n",
      "[90, 586, 209, 197, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "Do you like me?  -> Good, thanks.\n",
      "[664, 559, 414, 264, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "You're so mean  -> I'm quite happy, thank you.\n",
      "[525, 586, 235, 118, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "Can you drive?  -> Noted.\n",
      "[253, 141, 414, 31, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "That's so bad  -> No problem.\n",
      "[73, 586, 264, 118, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "what do you mean?  -> I think I might have gotten lost there.\n",
      "[88, 685, 685, 685, 685, 685, 685, 685, 685, 685, 685]\n",
      "oh my god  -> I've heard of other bots, but I haven't met any.\n"
     ]
    }
   ],
   "source": [
    "### Evaluation\n",
    "\n",
    "# Answer the question using the trained model\n",
    "def answer(sentence):\n",
    "    \n",
    "    seq_data = [sentence, '_U_ ' * max_output_words_amount]\n",
    "\n",
    "    input_batch, output_batch, target_batch = make_batch([seq_data])\n",
    "    \n",
    "    prediction = tf.argmax(model, 2)\n",
    "\n",
    "    result = sess.run(prediction,\n",
    "                      feed_dict={enc_input: input_batch,\n",
    "                                 dec_input: output_batch,\n",
    "                                 targets: target_batch})\n",
    "\n",
    "    # convert index number to actual token \n",
    "    decoded = [unique_words[i] for i in result[0]]\n",
    "        \n",
    "    # Remove anything after '_E_'        \n",
    "    if \"_E_\" in decoded:\n",
    "        end = decoded.index('_E_')\n",
    "        translated = ' '.join(decoded[:end])\n",
    "    else :\n",
    "        translated = ' '.join(decoded[:])\n",
    "    \n",
    "    return translated\n",
    "\n",
    "questions = [\"Hello\",\"I am so lonely\", \"Can you sleep?\", \"What is your age?\", \"I hate you\", \"Do you like me?\", \"You're so mean\", \"Can you drive?\", \"That's so bad\", \"what do you mean?\", \"oh my god\"]\n",
    "for q in questions:\n",
    "    print(q , ' ->', answer(q))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tgDIypjS07yD"
   },
   "source": [
    "\n",
    "### Output \n",
    "\n",
    "```\n",
    "Hello  -> Hello.\n",
    "I am so lonely  -> Okay.\n",
    "Can you sleep?  -> I don't have a body.\n",
    "What is your age?  -> Age doesn't really apply to me. \n",
    "I hate you  -> I'm sorry to hear that.\n",
    "Do you like me?  -> I do like you.\n",
    "You're so mean  -> I aim for efficiency.\n",
    "That's so bad  -> It's nice to have things you love.\n",
    "what do you mean?  -> Sorry about that.\n",
    "oh my god  -> I hope you're able to get some rest soon.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YsFtFn_-NRcW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "460087892zixu9636Lab4.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
